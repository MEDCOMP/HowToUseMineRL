{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a0e41c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankitagarg/.local/lib/python3.6/site-packages/gym/logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from agent import Agent\n",
    "from minecraft import DummyMinecraft, Env, test_policy\n",
    "from dataset import Dataset, Transition\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "from os.path import join as p_join\n",
    "from os.path import exists as p_exists\n",
    "\n",
    "from data_manager import StateManager, ActionManager\n",
    "\n",
    "from get_dataset import put_data_into_dataset\n",
    "\n",
    "import minerl\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8349ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ModuleNotFoundError:\n",
    "    from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e754b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2bool(v):\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1bf5e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = '/home/ankitagarg/minerl/minerl_imitation_learning/output/'\n",
    "DATASET_DIR = '/home/ankitagarg/minerl/data/'\n",
    "\n",
    "enable_cudnn = True\n",
    "train = True\n",
    "c_action_magnitude = 22.5 #magnitude of discretized camera action\n",
    "seed = 123\n",
    "scale_rewards = True\n",
    "\n",
    "learning_rate = 0.0000625\n",
    "adam_eps = 1.5e-4\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# parser.add_argument(\"--logdir\", default=\".\", type=str, help=\"used for logging and to save network snapshots\")\n",
    "net = 'deep_resnet'\n",
    "hidden_size = 1024\n",
    "dataset_path = None\n",
    "               \n",
    "trainsteps = 3000000\n",
    "augment_flip = True\n",
    "\n",
    "dataset_only_successful = False\n",
    "dataset_use_max_duration_steps = True\n",
    "dataset_continuous_action_stacking = 3\n",
    "dataset_max_reward = 256\n",
    "\n",
    "save_dataset_path = '/home/ankitagarg/minerl/minerl_imitation_learning/data/saved_dataset'\n",
    "quit_after_saving_dataset = False\n",
    "\n",
    "dueling = True\n",
    "\n",
    "add_treechop_data = False\n",
    "\n",
    "stop_time = 1\n",
    "test = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2279261",
   "metadata": {},
   "outputs": [],
   "source": [
    "if p_exists(p_join(OUTPUT_DIR, 'model_last.pth')):\n",
    "    print(\"Training already finished\")\n",
    "    train = False\n",
    "if p_exists(p_join(OUTPUT_DIR, \"tmp_time.p\")):\n",
    "    print(\"Detected tmp snapshot, will continue training from there\")\n",
    "    continue_from_tmp = True\n",
    "else:\n",
    "    continue_from_tmp = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3abf1da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n"
     ]
    }
   ],
   "source": [
    "#Setup\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(np.random.randint(1, 10000))\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "torch.cuda.manual_seed(np.random.randint(1, 10000))\n",
    "torch.backends.cudnn.enabled = enable_cudnn\n",
    "device = torch.device('cuda')\n",
    "\n",
    "print(f\"Running on {device}\")\n",
    "\n",
    "state_manager = StateManager(device)\n",
    "action_manager = ActionManager(device, c_action_magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "446f772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(OUTPUT_DIR)\n",
    "\n",
    "with open(p_join(OUTPUT_DIR, \"status.txt\"), 'w') as status_file:\n",
    "    status_file.write('running')\n",
    "\n",
    "# extended error exception:\n",
    "def handle_exception(exc_type, exc_value, exc_traceback):\n",
    "\n",
    "    with open(p_join(OUTPUT_DIR, \"status.txt\"), 'w') as status_file_:\n",
    "        status_file_.write('error')\n",
    "\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    env.close()\n",
    "    sys.__excepthook__(exc_type, exc_value, exc_traceback)\n",
    "\n",
    "sys.excepthook = handle_exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6398a5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankitagarg/.local/lib/python3.6/site-packages/gym/logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started env\n",
      "env reset\n",
      "img, vec shapes:  torch.Size([1, 3, 64, 64]) torch.Size([1, 216])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankitagarg/minerl/minerl_imitation_learning/data_manager.py:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1640811805959/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  img_torch = torch.tensor(img_list, dtype=torch.float32, device=self.device).div_(255).permute(0, 3, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "#create the environment\n",
    "env_ = DummyMinecraft()\n",
    "env_.seed(seed)\n",
    "\n",
    "env = Env(env_, state_manager, action_manager)\n",
    "\n",
    "print(\"started env\")\n",
    "\n",
    "img, vec = env.reset()\n",
    "\n",
    "print(\"env reset\")\n",
    "\n",
    "print(\"img, vec shapes: \", img.shape, vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f2afa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = action_manager.num_action_ids_list[0]\n",
    "image_channels = img.shape[1]\n",
    "\n",
    "vec_size = vec.shape[1]\n",
    "vec_shape = vec.shape[1:]\n",
    "\n",
    "img_shape = list(img.shape[1:])\n",
    "img_shape[0] = int(img_shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af9c2ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating dataset\n",
      "\n",
      " Adding data from MineRLObtainIronPickaxe-v0 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 664/3213 [00:00<00:00, 3509.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'success': False, 'duration_ms': 160650, 'duration_steps': 3213, 'total_reward': 547.0, 'stream_name': 'v3_juvenile_apple_angel-7_212895-216138', 'true_video_frame_count': 3244}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3213/3213 [00:00<00:00, 3478.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 2, added: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 450/3965 [00:00<00:01, 2244.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'success': False, 'duration_ms': 198250, 'duration_steps': 3965, 'total_reward': 547.0, 'stream_name': 'v3_sticky_chick_pea_gnome-21_46603-50686', 'true_video_frame_count': 4085}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3965/3965 [00:01<00:00, 3147.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 / 2, added: 2\n",
      "\n",
      " Adding data from MineRLObtainDiamond-v0 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 566/69526 [00:00<00:12, 5656.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'success': False, 'duration_ms': 3476300, 'duration_steps': 69526, 'total_reward': 99.0, 'stream_name': 'v3_self_reliant_fig_doppelganger-1_37451-107047', 'true_video_frame_count': 69598}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69526/69526 [00:17<00:00, 3927.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 2, added: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2018 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'success': False, 'duration_ms': 100900, 'duration_steps': 2018, 'total_reward': 35.0, 'stream_name': 'v3_key_nectarine_spirit-1_1619-3682', 'true_video_frame_count': 2065}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2018/2018 [00:00<00:00, 2896.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 / 2, added: 2\n",
      "saved new dataset/home/ankitagarg/minerl/minerl_imitation_learning/data/saved_dataset with 17955 transitions\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(device, 2000000, img_shape, vec_shape,\n",
    "                  state_manager, action_manager,\n",
    "                  scale_rewards=scale_rewards)\n",
    "    \n",
    "if dataset_path is not None:  # default None\n",
    "\n",
    "    print(f\"loading dataset {dataset_path}\")\n",
    "    dataset.load(dataset_path)\n",
    "    print(f\"loaded dataset\")\n",
    "\n",
    "else:  # creating dataset:\n",
    "\n",
    "    assert DATASET_DIR is not None\n",
    "\n",
    "    print(\"creating dataset\")\n",
    "\n",
    "    if dataset_use_max_duration_steps:  # default: True\n",
    "        max_iron_pickaxe_duration = 6000\n",
    "        max_diamond_duration = 18000\n",
    "    else:\n",
    "        max_iron_pickaxe_duration = None\n",
    "        max_diamond_duration = None\n",
    "\n",
    "    put_data_into_dataset(\n",
    "        'MineRLObtainIronPickaxe-v0', action_manager, dataset, DATASET_DIR,\n",
    "        dataset_continuous_action_stacking,\n",
    "        dataset_only_successful,\n",
    "        max_iron_pickaxe_duration,\n",
    "        dataset_max_reward,\n",
    "        test)\n",
    "\n",
    "    put_data_into_dataset(\n",
    "        'MineRLObtainDiamond-v0', action_manager, dataset, DATASET_DIR,\n",
    "        dataset_continuous_action_stacking,\n",
    "        dataset_only_successful,\n",
    "        max_diamond_duration,\n",
    "        dataset_max_reward,\n",
    "        test)\n",
    "\n",
    "    if add_treechop_data:\n",
    "        put_data_into_dataset(\n",
    "            'MineRLTreechop-v0', action_manager, dataset, DATASET_DIR,\n",
    "            dataset_continuous_action_stacking,\n",
    "            dataset_only_successful,\n",
    "            None,\n",
    "            dataset_max_reward,\n",
    "            test)\n",
    "\n",
    "    if save_dataset_path is not None:\n",
    "        dataset.save(save_dataset_path)\n",
    "        print(f\"saved new dataset{save_dataset_path} with {dataset.transitions.index} transitions\")\n",
    "        \n",
    "    else:\n",
    "        print(\"continuing with new dataset without saving\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46448d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(dataset.transitions.index):\n",
    "    dataset.transitions.data[j] = Transition(\n",
    "        dataset.transitions.data[j].state.pin_memory(),\n",
    "        dataset.transitions.data[j].vector.pin_memory(),\n",
    "        dataset.transitions.data[j].action,\n",
    "        dataset.transitions.data[j].reward,\n",
    "        dataset.transitions.data[j].nonterminal\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ea1aa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(num_actions, image_channels, vec_size, writer,\n",
    "              net, batch_size, augment_flip, hidden_size, dueling,\n",
    "              learning_rate, adam_eps, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9e472df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000023639202118 h passed, saving tmp snapshot\n",
      "saved\n"
     ]
    }
   ],
   "source": [
    "init_time = time.time()\n",
    "if continue_from_tmp:\n",
    "    start_int = pickle.load(open(p_join(OUTPUT_DIR, \"tmp_time.p\"), \"rb\"))\n",
    "    print(f\"continuing from {start_int} trainstep\")\n",
    "    agent.load(OUTPUT_DIR, \"tmp\")\n",
    "else:\n",
    "    start_int = 0\n",
    "\n",
    "agent.train()\n",
    "\n",
    "with open(p_join(OUTPUT_DIR, \"status.txt\"), 'w') as status_file:\n",
    "    status_file.write('running training')\n",
    "\n",
    "if test:\n",
    "    trainsteps = 10\n",
    "\n",
    "fps_t0 = time.time()\n",
    "\n",
    "for i in range(start_int, trainsteps):\n",
    "\n",
    "    agent.learn(i, dataset, write=(i % 1000 == 0))\n",
    "\n",
    "    if i and i % 100000 == 0:\n",
    "        agent.save(OUTPUT_DIR, i // 100000)\n",
    "\n",
    "    if stop_time is not None:\n",
    "        if ((time.time() - init_time) / 60. / 60.) > stop_time:\n",
    "            print(f\"{(time.time() - init_time) / 60. / 60.} h passed, saving tmp snapshot\", flush=True)\n",
    "            agent.save(OUTPUT_DIR, \"tmp\")\n",
    "            pickle.dump(int(i), open(p_join(OUTPUT_DIR, \"tmp_time.p\"), 'wb'))\n",
    "            writer.close()\n",
    "            print('saved')\n",
    "            break\n",
    "\n",
    "    if (i+1) % 5000 == 0:\n",
    "        fps = float(i - start_int) / (time.time() - fps_t0)\n",
    "        writer.add_scalar(\"fps\", fps, i)\n",
    "\n",
    "agent.save(OUTPUT_DIR, 'last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdaeeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def put_data_into_dataset(env_name, action_manager, dataset, minecraft_human_data_dir,\n",
    "#                           continuous_action_stacking_amount=3,\n",
    "#                           only_successful=True, max_duration_steps=None, max_reward=256.,\n",
    "#                           test=False):\n",
    "#     \"\"\"\n",
    "#     :param env_name: Minecraft env name\n",
    "#     :param action_manager: expects object of data_manager.ActionManager\n",
    "#     :param dataset: expects object of dataset.Dataset\n",
    "#     :param minecraft_human_data_dir: location of Minecraft human data\n",
    "#     :param continuous_action_stacking_amount: number of consecutive states that are used to get the continuous action\n",
    "#     (since humans move the camera slowly we add up the continuous actions of multiple consecutive states)\n",
    "#     :param only_successful: skip trajectories that don't reach final reward when true\n",
    "#     :param max_duration_steps: skip trajectories that take longer than max_duration_steps to reach the final reward\n",
    "#     :param max_reward: remove trajectory part beyond the max_reward. Used to remove the \"obtain diamond\" part, since\n",
    "#     the imitation policy never obtains diamonds anyway\n",
    "#     :param test: if true a mini dataset is created for debugging\n",
    "\n",
    "#     further all samples without rewards, and without terminal states, and with no_op action are removed\n",
    "#     \"\"\"\n",
    "\n",
    "#     print(f\"\\n Adding data from {env_name} \\n\")\n",
    "\n",
    "#     treechop_data = env_name == \"MineRLTreechop-v0\"\n",
    "\n",
    "#     def is_success(sample):\n",
    "#         if max_duration_steps is None:\n",
    "#             return sample[-1]['success']\n",
    "#         else:\n",
    "#             return sample[-1]['success'] and sample[-1]['duration_steps'] < max_duration_steps\n",
    "\n",
    "#     def is_no_op(sample):\n",
    "#         action = sample[1]\n",
    "#         a_id = action_manager.get_id(action)\n",
    "#         assert type(a_id) == int\n",
    "#         return a_id == 0  # no_op action has id of 0\n",
    "\n",
    "#     def process_sample(sample, last_reward):\n",
    "#         \"\"\"adding single sample to dataset if all conditions are met, expects sample with already stacked\n",
    "#         camera action\"\"\"\n",
    "\n",
    "#         reward = sample[2]\n",
    "\n",
    "#         if reward > last_reward:\n",
    "#             last_reward = reward\n",
    "\n",
    "#         gatherlog_sample = last_reward < 2.\n",
    "\n",
    "#         if treechop_data:\n",
    "#             # fill missing action and state parts with zeros:\n",
    "#             for key, value in action_manager.zero_action.items():\n",
    "#                 if key not in sample[1]:\n",
    "#                     sample[1][key] = value\n",
    "\n",
    "#             sample[0]['equipped_items'] = OrderedDict([(\n",
    "#                 'mainhand',\n",
    "#                 OrderedDict([('damage', 0), ('maxDamage', 0), ('type', 0)])\n",
    "#             )])\n",
    "\n",
    "#             sample[0][\"inventory\"] = OrderedDict([\n",
    "#                 ('coal', 0),\n",
    "#                 ('cobblestone', 0),\n",
    "#                 ('crafting_table', 0),\n",
    "#                 ('dirt', 0),\n",
    "#                 ('furnace', 0),\n",
    "#                 ('iron_axe', 0),\n",
    "#                 ('iron_ingot', 0),\n",
    "#                 ('iron_ore', 0),\n",
    "#                 ('iron_pickaxe', 0),\n",
    "#                 ('log', 0),\n",
    "#                 ('planks', 0),\n",
    "#                 ('stick', 0),\n",
    "#                 ('stone', 0),\n",
    "#                 ('stone_axe', 0),\n",
    "#                 ('stone_pickaxe', 0),\n",
    "#                 ('torch', 0),\n",
    "#                 ('wooden_axe', 0),\n",
    "#                 ('wooden_pickaxe', 0)\n",
    "#             ])\n",
    "\n",
    "#         if reward != 0.:\n",
    "#             if reward > max_reward:\n",
    "#                 # if a larger reward is encountered, the transition is deleted until previous reward:\n",
    "#                 counter_change = - dataset.remove_new_data()\n",
    "#             else:\n",
    "#                 dataset.append_sample(sample, gatherlog_sample, treechop_data)\n",
    "#                 dataset.update_last_reward_index()\n",
    "#                 counter_change = 1\n",
    "#         else:\n",
    "#             if not is_no_op(sample) or sample[4]:  # remove no_op transitions, unless it is a terminal state\n",
    "#                 dataset.append_sample(sample, gatherlog_sample, treechop_data)\n",
    "#                 counter_change = 1\n",
    "#             else:\n",
    "#                 counter_change = 0\n",
    "\n",
    "#         return counter_change, last_reward\n",
    "\n",
    "#     data = minerl.data.make(env_name, data_dir=minecraft_human_data_dir)\n",
    "#     trajs = data.get_trajectory_names()\n",
    "\n",
    "#     # the ring buffer is used to stack the camera action of multiple consecutive states:\n",
    "#     sample_que = deque(maxlen=continuous_action_stacking_amount)\n",
    "\n",
    "#     total_trajs_counter = 0\n",
    "#     added_sample_counter = 0\n",
    "\n",
    "#     initial_sample_amount = dataset.transitions.current_size()\n",
    "\n",
    "#     for n, traj in enumerate(trajs):\n",
    "#         for j, sample in enumerate(data.load_data(traj, include_metadata=True)):\n",
    "\n",
    "#             # checking if the trajectory will be used first:\n",
    "#             if j == 0:\n",
    "#                 print(sample[-1])\n",
    "\n",
    "#                 if only_successful:\n",
    "#                     if not is_success(sample):\n",
    "#                         print(\"skipping trajectory\")\n",
    "#                         break\n",
    "\n",
    "#                 total_trajs_counter += 1\n",
    "#                 last_reward = 0.\n",
    "\n",
    "#             sample_que.append(sample)\n",
    "\n",
    "#             # Only continue when we have enough states to stack the camera actions:\n",
    "#             if len(sample_que) == continuous_action_stacking_amount:\n",
    "\n",
    "#                 # Stacking camera action for the oldest sample in the queue:\n",
    "#                 for i in range(1, continuous_action_stacking_amount):\n",
    "#                     sample_que[0][1]['camera'] += sample_que[i][1]['camera']\n",
    "\n",
    "#                     if sample_que[i][2] != 0.:  # (if reward != 0)\n",
    "#                         break  # no camera action stacking after a reward\n",
    "\n",
    "#                 added_samples, last_reward = process_sample(sample_que[0], last_reward)\n",
    "\n",
    "#                 added_sample_counter += added_samples\n",
    "\n",
    "#         if len(sample_que) > 0:  # otherwise not successful traj\n",
    "#             # for the last samples in the queue we don't stack the the camera actions\n",
    "#             for i in range(1, continuous_action_stacking_amount):\n",
    "#                 added_samples, last_reward = process_sample(sample_que[i], last_reward)\n",
    "#                 added_sample_counter += added_samples\n",
    "\n",
    "#             # a terminal state could be reached without exceeding max_reward:\n",
    "#             added_sample_counter -= dataset.remove_new_data()\n",
    "\n",
    "#             # making sure the last state from trajectory is terminal:\n",
    "#             last_transition = deepcopy(dataset.transitions.data[dataset.transitions.index - 1])\n",
    "#             dataset.transitions.data[dataset.transitions.index - 1] = \\\n",
    "#                 Transition(last_transition.state, last_transition.vector,\n",
    "#                            last_transition.action, last_transition.reward, False)\n",
    "\n",
    "#         sample_que.clear()\n",
    "\n",
    "#         print(f\"{n+1} / {len(trajs)}, added: {total_trajs_counter}\")\n",
    "#         assert dataset.transitions.current_size() - initial_sample_amount == added_sample_counter\n",
    "\n",
    "#         if test:\n",
    "#             if total_trajs_counter >= 2:\n",
    "#                 assert total_trajs_counter == 2\n",
    "#                 break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba218dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:venv] *",
   "language": "python",
   "name": "conda-env-venv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
