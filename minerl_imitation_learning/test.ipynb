{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c9caddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankitagarg/.local/lib/python3.6/site-packages/gym/logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from agent import Agent\n",
    "from minecraft import DummyMinecraft, Env, test_policy\n",
    "from dataset import Dataset, Transition\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "from os.path import join as p_join\n",
    "from os.path import exists as p_exists\n",
    "\n",
    "from data_manager import StateManager, ActionManager\n",
    "\n",
    "from get_dataset import put_data_into_dataset\n",
    "\n",
    "import minerl\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdd9750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ModuleNotFoundError:\n",
    "    from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc73135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2bool(v):\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2e77d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = '/home/ankitagarg/minerl/minerl_imitation_learning/test_output/'\n",
    "DATASET_DIR = '/home/ankitagarg/minerl/data/'\n",
    "\n",
    "enable_cudnn = True\n",
    "train = True\n",
    "c_action_magnitude = 22.5 #magnitude of discretized camera action\n",
    "seed = 123\n",
    "scale_rewards = True\n",
    "\n",
    "learning_rate = 0.0000625\n",
    "adam_eps = 1.5e-4\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# parser.add_argument(\"--logdir\", default=\".\", type=str, help=\"used for logging and to save network snapshots\")\n",
    "net = 'deep_resnet'\n",
    "hidden_size = 1024\n",
    "dataset_path = None\n",
    "               \n",
    "trainsteps = 3000000\n",
    "augment_flip = True\n",
    "\n",
    "dataset_only_successful = False\n",
    "dataset_use_max_duration_steps = True\n",
    "dataset_continuous_action_stacking = 3\n",
    "dataset_max_reward = 256\n",
    "\n",
    "save_dataset_path = '/home/ankitagarg/minerl/minerl_imitation_learning/data/saved_dataset'\n",
    "quit_after_saving_dataset = False\n",
    "\n",
    "dueling = True\n",
    "\n",
    "add_treechop_data = False\n",
    "\n",
    "stop_time = None\n",
    "test = False\n",
    "\n",
    "eval_policy_path='/home/ankitagarg/minerl/minerl_imitation_learning/output_2/'\n",
    "eval_policy_model_id=\"last\"\n",
    "eval_policy_episodes=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34399b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n"
     ]
    }
   ],
   "source": [
    "#Setup\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(np.random.randint(1, 10000))\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "torch.cuda.manual_seed(np.random.randint(1, 10000))\n",
    "torch.backends.cudnn.enabled = enable_cudnn\n",
    "device = torch.device('cuda')\n",
    "\n",
    "print(f\"Running on {device}\")\n",
    "\n",
    "state_manager = StateManager(device)\n",
    "action_manager = ActionManager(device, c_action_magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54fba7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(400, 300))\n",
    "display.start();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3ef56a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colabgymrender.recorder import Recorder\n",
    "env_ = gym.make('MineRLObtainIronPickaxe-v0')\n",
    "env_.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbf21115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started env\n",
      "env reset\n",
      "img, vec shapes:  torch.Size([1, 3, 64, 64]) torch.Size([1, 216])\n"
     ]
    }
   ],
   "source": [
    "env = Env(env_, state_manager, action_manager)\n",
    "# env = Recorder(env, './video', fps=60)\n",
    "\n",
    "print(\"started env\")\n",
    "\n",
    "img, vec = env.reset()\n",
    "\n",
    "print(\"env reset\")\n",
    "\n",
    "print(\"img, vec shapes: \", img.shape, vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1e5a60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = action_manager.num_action_ids_list[0]\n",
    "image_channels = img.shape[1]\n",
    "\n",
    "vec_size = vec.shape[1]\n",
    "vec_shape = vec.shape[1:]\n",
    "\n",
    "img_shape = list(img.shape[1:])\n",
    "img_shape[0] = int(img_shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2c6bc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(OUTPUT_DIR)\n",
    "\n",
    "with open(p_join(OUTPUT_DIR, \"status.txt\"), 'w') as status_file:\n",
    "    status_file.write('running')\n",
    "\n",
    "# extended error exception:\n",
    "def handle_exception(exc_type, exc_value, exc_traceback):\n",
    "\n",
    "    with open(p_join(OUTPUT_DIR, \"status.txt\"), 'w') as status_file_:\n",
    "        status_file_.write('error')\n",
    "\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    env.close()\n",
    "    sys.__excepthook__(exc_type, exc_value, exc_traceback)\n",
    "\n",
    "sys.excepthook = handle_exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b3cfcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(num_actions, image_channels, vec_size, writer,\n",
    "              net, batch_size, augment_flip, hidden_size, dueling,\n",
    "              learning_rate, adam_eps, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b06b7773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded network /home/ankitagarg/minerl/minerl_imitation_learning/output_2/ last\n",
      "episode 0\n",
      "episode reward: 0.0 , episode terminated after 2036 env steps\n",
      "avg_reward after 1 (out of 100) episodes: 0.0\n",
      "episode 1\n",
      "episode reward: 0.0 , episode terminated after 1124 env steps\n",
      "avg_reward after 2 (out of 100) episodes: 0.0\n",
      "episode 2\n",
      "episode reward: 1.0 , episode terminated after 5990 env steps\n",
      "avg_reward after 3 (out of 100) episodes: 0.3333333333333333\n",
      "episode 3\n",
      "episode reward: 0.0 , episode terminated after 5989 env steps\n",
      "avg_reward after 4 (out of 100) episodes: 0.25\n",
      "episode 4\n",
      "episode reward: 1.0 , episode terminated after 5987 env steps\n",
      "avg_reward after 5 (out of 100) episodes: 0.4\n",
      "episode 5\n",
      "episode reward: 0.0 , episode terminated after 5989 env steps\n",
      "avg_reward after 6 (out of 100) episodes: 0.3333333333333333\n",
      "episode 6\n",
      "episode reward: 0.0 , episode terminated after 942 env steps\n",
      "avg_reward after 7 (out of 100) episodes: 0.2857142857142857\n",
      "episode 7\n",
      "episode reward: 7.0 , episode terminated after 5991 env steps\n",
      "avg_reward after 8 (out of 100) episodes: 1.125\n",
      "episode 8\n",
      "episode reward: 0.0 , episode terminated after 977 env steps\n",
      "avg_reward after 9 (out of 100) episodes: 1.0\n",
      "episode 9\n",
      "episode reward: 0.0 , episode terminated after 936 env steps\n",
      "avg_reward after 10 (out of 100) episodes: 0.9\n",
      "episode 10\n",
      "episode reward: 0.0 , episode terminated after 5991 env steps\n",
      "avg_reward after 11 (out of 100) episodes: 0.8181818181818182\n",
      "episode 11\n",
      "episode reward: 0.0 , episode terminated after 5988 env steps\n",
      "avg_reward after 12 (out of 100) episodes: 0.75\n",
      "episode 12\n",
      "episode reward: 0.0 , episode terminated after 4585 env steps\n",
      "avg_reward after 13 (out of 100) episodes: 0.6923076923076923\n",
      "episode 13\n",
      "episode reward: 0.0 , episode terminated after 879 env steps\n",
      "avg_reward after 14 (out of 100) episodes: 0.6428571428571429\n",
      "episode 14\n",
      "episode reward: 0.0 , episode terminated after 971 env steps\n",
      "avg_reward after 15 (out of 100) episodes: 0.6\n",
      "episode 15\n",
      "episode reward: 0.0 , episode terminated after 697 env steps\n",
      "avg_reward after 16 (out of 100) episodes: 0.5625\n",
      "episode 16\n",
      "episode reward: 0.0 , episode terminated after 2327 env steps\n",
      "avg_reward after 17 (out of 100) episodes: 0.5294117647058824\n",
      "episode 17\n",
      "episode reward: 0.0 , episode terminated after 3346 env steps\n",
      "avg_reward after 18 (out of 100) episodes: 0.5\n",
      "episode 18\n",
      "episode reward: 0.0 , episode terminated after 644 env steps\n",
      "avg_reward after 19 (out of 100) episodes: 0.47368421052631576\n",
      "episode 19\n",
      "episode reward: 0.0 , episode terminated after 732 env steps\n",
      "avg_reward after 20 (out of 100) episodes: 0.45\n",
      "episode 20\n",
      "episode reward: 0.0 , episode terminated after 5982 env steps\n",
      "avg_reward after 21 (out of 100) episodes: 0.42857142857142855\n",
      "episode 21\n",
      "episode reward: 7.0 , episode terminated after 5978 env steps\n",
      "avg_reward after 22 (out of 100) episodes: 0.7272727272727273\n",
      "episode 22\n",
      "episode reward: 0.0 , episode terminated after 5989 env steps\n",
      "avg_reward after 23 (out of 100) episodes: 0.6956521739130435\n",
      "episode 23\n",
      "episode reward: 0.0 , episode terminated after 348 env steps\n",
      "avg_reward after 24 (out of 100) episodes: 0.6666666666666666\n",
      "episode 24\n",
      "episode reward: 0.0 , episode terminated after 1568 env steps\n",
      "avg_reward after 25 (out of 100) episodes: 0.64\n",
      "episode 25\n",
      "episode reward: 0.0 , episode terminated after 5975 env steps\n",
      "avg_reward after 26 (out of 100) episodes: 0.6153846153846154\n",
      "episode 26\n",
      "episode reward: 0.0 , episode terminated after 1699 env steps\n",
      "avg_reward after 27 (out of 100) episodes: 0.5925925925925926\n",
      "episode 27\n",
      "episode reward: 0.0 , episode terminated after 931 env steps\n",
      "avg_reward after 28 (out of 100) episodes: 0.5714285714285714\n",
      "episode 28\n",
      "episode reward: 0.0 , episode terminated after 2793 env steps\n",
      "avg_reward after 29 (out of 100) episodes: 0.5517241379310345\n",
      "episode 29\n",
      "episode reward: 0.0 , episode terminated after 749 env steps\n",
      "avg_reward after 30 (out of 100) episodes: 0.5333333333333333\n",
      "episode 30\n",
      "episode reward: 0.0 , episode terminated after 948 env steps\n",
      "avg_reward after 31 (out of 100) episodes: 0.5161290322580645\n",
      "episode 31\n",
      "episode reward: 0.0 , episode terminated after 5979 env steps\n",
      "avg_reward after 32 (out of 100) episodes: 0.5\n",
      "episode 32\n",
      "episode reward: 0.0 , episode terminated after 643 env steps\n",
      "avg_reward after 33 (out of 100) episodes: 0.48484848484848486\n",
      "episode 33\n",
      "episode reward: 0.0 , episode terminated after 5991 env steps\n",
      "avg_reward after 34 (out of 100) episodes: 0.47058823529411764\n",
      "episode 34\n",
      "episode reward: 0.0 , episode terminated after 915 env steps\n",
      "avg_reward after 35 (out of 100) episodes: 0.45714285714285713\n",
      "episode 35\n",
      "episode reward: 0.0 , episode terminated after 920 env steps\n",
      "avg_reward after 36 (out of 100) episodes: 0.4444444444444444\n",
      "episode 36\n",
      "episode reward: 0.0 , episode terminated after 5990 env steps\n",
      "avg_reward after 37 (out of 100) episodes: 0.43243243243243246\n",
      "episode 37\n",
      "episode reward: 0.0 , episode terminated after 5984 env steps\n",
      "avg_reward after 38 (out of 100) episodes: 0.42105263157894735\n",
      "episode 38\n",
      "episode reward: 3.0 , episode terminated after 5989 env steps\n",
      "avg_reward after 39 (out of 100) episodes: 0.48717948717948717\n",
      "episode 39\n",
      "episode reward: 0.0 , episode terminated after 666 env steps\n",
      "avg_reward after 40 (out of 100) episodes: 0.475\n",
      "episode 40\n",
      "episode reward: 0.0 , episode terminated after 5989 env steps\n",
      "avg_reward after 41 (out of 100) episodes: 0.4634146341463415\n",
      "episode 41\n",
      "episode reward: 0.0 , episode terminated after 1090 env steps\n",
      "avg_reward after 42 (out of 100) episodes: 0.4523809523809524\n",
      "episode 42\n",
      "episode reward: 0.0 , episode terminated after 5978 env steps\n",
      "avg_reward after 43 (out of 100) episodes: 0.4418604651162791\n",
      "episode 43\n",
      "episode reward: 0.0 , episode terminated after 921 env steps\n",
      "avg_reward after 44 (out of 100) episodes: 0.4318181818181818\n",
      "episode 44\n",
      "episode reward: 7.0 , episode terminated after 4446 env steps\n",
      "avg_reward after 45 (out of 100) episodes: 0.5777777777777777\n",
      "episode 45\n",
      "episode reward: 0.0 , episode terminated after 1043 env steps\n",
      "avg_reward after 46 (out of 100) episodes: 0.5652173913043478\n",
      "episode 46\n",
      "episode reward: 0.0 , episode terminated after 5989 env steps\n",
      "avg_reward after 47 (out of 100) episodes: 0.5531914893617021\n",
      "episode 47\n",
      "episode reward: 0.0 , episode terminated after 816 env steps\n",
      "avg_reward after 48 (out of 100) episodes: 0.5416666666666666\n",
      "episode 48\n",
      "episode reward: 0.0 , episode terminated after 4645 env steps\n",
      "avg_reward after 49 (out of 100) episodes: 0.5306122448979592\n",
      "episode 49\n",
      "episode reward: 0.0 , episode terminated after 5999 env steps\n",
      "avg_reward after 50 (out of 100) episodes: 0.52\n",
      "episode 50\n",
      "episode reward: 0.0 , episode terminated after 5980 env steps\n",
      "avg_reward after 51 (out of 100) episodes: 0.5098039215686274\n",
      "episode 51\n",
      "episode reward: 1.0 , episode terminated after 5979 env steps\n",
      "avg_reward after 52 (out of 100) episodes: 0.5192307692307693\n",
      "episode 52\n",
      "episode reward: 0.0 , episode terminated after 5332 env steps\n",
      "avg_reward after 53 (out of 100) episodes: 0.5094339622641509\n",
      "episode 53\n",
      "episode reward: 0.0 , episode terminated after 643 env steps\n",
      "avg_reward after 54 (out of 100) episodes: 0.5\n",
      "episode 54\n",
      "episode reward: 0.0 , episode terminated after 5983 env steps\n",
      "avg_reward after 55 (out of 100) episodes: 0.4909090909090909\n",
      "episode 55\n",
      "episode reward: 0.0 , episode terminated after 5977 env steps\n",
      "avg_reward after 56 (out of 100) episodes: 0.48214285714285715\n",
      "episode 56\n",
      "episode reward: 1.0 , episode terminated after 5974 env steps\n",
      "avg_reward after 57 (out of 100) episodes: 0.49122807017543857\n",
      "episode 57\n",
      "episode reward: 0.0 , episode terminated after 5981 env steps\n",
      "avg_reward after 58 (out of 100) episodes: 0.4827586206896552\n",
      "episode 58\n",
      "episode reward: 0.0 , episode terminated after 5984 env steps\n",
      "avg_reward after 59 (out of 100) episodes: 0.4745762711864407\n",
      "episode 59\n",
      "episode reward: 0.0 , episode terminated after 5975 env steps\n",
      "avg_reward after 60 (out of 100) episodes: 0.4666666666666667\n",
      "episode 60\n",
      "episode reward: 0.0 , episode terminated after 643 env steps\n",
      "avg_reward after 61 (out of 100) episodes: 0.45901639344262296\n",
      "episode 61\n",
      "episode reward: 0.0 , episode terminated after 1249 env steps\n",
      "avg_reward after 62 (out of 100) episodes: 0.45161290322580644\n",
      "episode 62\n",
      "episode reward: 0.0 , episode terminated after 1043 env steps\n",
      "avg_reward after 63 (out of 100) episodes: 0.4444444444444444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 63\n",
      "episode reward: 0.0 , episode terminated after 3748 env steps\n",
      "avg_reward after 64 (out of 100) episodes: 0.4375\n",
      "episode 64\n",
      "episode reward: 0.0 , episode terminated after 5984 env steps\n",
      "avg_reward after 65 (out of 100) episodes: 0.4307692307692308\n",
      "episode 65\n",
      "episode reward: 0.0 , episode terminated after 1044 env steps\n",
      "avg_reward after 66 (out of 100) episodes: 0.42424242424242425\n",
      "episode 66\n",
      "episode reward: 0.0 , episode terminated after 921 env steps\n",
      "avg_reward after 67 (out of 100) episodes: 0.417910447761194\n",
      "episode 67\n",
      "episode reward: 0.0 , episode terminated after 5987 env steps\n",
      "avg_reward after 68 (out of 100) episodes: 0.4117647058823529\n",
      "episode 68\n",
      "episode reward: 0.0 , episode terminated after 5982 env steps\n",
      "avg_reward after 69 (out of 100) episodes: 0.4057971014492754\n",
      "episode 69\n",
      "episode reward: 0.0 , episode terminated after 875 env steps\n",
      "avg_reward after 70 (out of 100) episodes: 0.4\n",
      "episode 70\n",
      "episode reward: 1.0 , episode terminated after 5992 env steps\n",
      "avg_reward after 71 (out of 100) episodes: 0.4084507042253521\n",
      "episode 71\n",
      "episode reward: 0.0 , episode terminated after 643 env steps\n",
      "avg_reward after 72 (out of 100) episodes: 0.4027777777777778\n",
      "episode 72\n",
      "episode reward: 0.0 , episode terminated after 5988 env steps\n",
      "avg_reward after 73 (out of 100) episodes: 0.3972602739726027\n",
      "episode 73\n",
      "episode reward: 0.0 , episode terminated after 1534 env steps\n",
      "avg_reward after 74 (out of 100) episodes: 0.3918918918918919\n",
      "episode 74\n",
      "episode reward: 0.0 , episode terminated after 1097 env steps\n",
      "avg_reward after 75 (out of 100) episodes: 0.38666666666666666\n",
      "episode 75\n",
      "episode reward: 0.0 , episode terminated after 5988 env steps\n",
      "avg_reward after 76 (out of 100) episodes: 0.3815789473684211\n",
      "episode 76\n",
      "episode reward: 0.0 , episode terminated after 644 env steps\n",
      "avg_reward after 77 (out of 100) episodes: 0.37662337662337664\n",
      "episode 77\n",
      "episode reward: 0.0 , episode terminated after 1294 env steps\n",
      "avg_reward after 78 (out of 100) episodes: 0.3717948717948718\n",
      "episode 78\n",
      "episode reward: 0.0 , episode terminated after 520 env steps\n",
      "avg_reward after 79 (out of 100) episodes: 0.3670886075949367\n",
      "episode 79\n",
      "episode reward: 0.0 , episode terminated after 5989 env steps\n",
      "avg_reward after 80 (out of 100) episodes: 0.3625\n",
      "episode 80\n",
      "episode reward: 0.0 , episode terminated after 5991 env steps\n",
      "avg_reward after 81 (out of 100) episodes: 0.35802469135802467\n",
      "episode 81\n",
      "episode reward: 0.0 , episode terminated after 5979 env steps\n",
      "avg_reward after 82 (out of 100) episodes: 0.35365853658536583\n",
      "episode 82\n",
      "episode reward: 0.0 , episode terminated after 644 env steps\n",
      "avg_reward after 83 (out of 100) episodes: 0.3493975903614458\n",
      "episode 83\n",
      "episode reward: 1.0 , episode terminated after 5999 env steps\n",
      "avg_reward after 84 (out of 100) episodes: 0.35714285714285715\n",
      "episode 84\n",
      "episode reward: 1.0 , episode terminated after 5984 env steps\n",
      "avg_reward after 85 (out of 100) episodes: 0.36470588235294116\n",
      "episode 85\n",
      "episode reward: 0.0 , episode terminated after 1052 env steps\n",
      "avg_reward after 86 (out of 100) episodes: 0.36046511627906974\n",
      "episode 86\n",
      "episode reward: 0.0 , episode terminated after 5981 env steps\n",
      "avg_reward after 87 (out of 100) episodes: 0.3563218390804598\n",
      "episode 87\n",
      "episode reward: 0.0 , episode terminated after 5990 env steps\n",
      "avg_reward after 88 (out of 100) episodes: 0.3522727272727273\n",
      "episode 88\n",
      "episode reward: 3.0 , episode terminated after 5981 env steps\n",
      "avg_reward after 89 (out of 100) episodes: 0.38202247191011235\n",
      "episode 89\n",
      "episode reward: 0.0 , episode terminated after 5986 env steps\n",
      "avg_reward after 90 (out of 100) episodes: 0.37777777777777777\n",
      "episode 90\n",
      "episode reward: 0.0 , episode terminated after 5989 env steps\n",
      "avg_reward after 91 (out of 100) episodes: 0.37362637362637363\n",
      "episode 91\n",
      "episode reward: 0.0 , episode terminated after 5981 env steps\n",
      "avg_reward after 92 (out of 100) episodes: 0.3695652173913043\n",
      "episode 92\n",
      "episode reward: 0.0 , episode terminated after 5984 env steps\n",
      "avg_reward after 93 (out of 100) episodes: 0.3655913978494624\n",
      "episode 93\n",
      "episode reward: 0.0 , episode terminated after 5989 env steps\n",
      "avg_reward after 94 (out of 100) episodes: 0.3617021276595745\n",
      "episode 94\n",
      "episode reward: 1.0 , episode terminated after 5988 env steps\n",
      "avg_reward after 95 (out of 100) episodes: 0.3684210526315789\n",
      "episode 95\n",
      "episode reward: 0.0 , episode terminated after 5979 env steps\n",
      "avg_reward after 96 (out of 100) episodes: 0.3645833333333333\n",
      "episode 96\n",
      "episode reward: 0.0 , episode terminated after 5996 env steps\n",
      "avg_reward after 97 (out of 100) episodes: 0.36082474226804123\n",
      "episode 97\n",
      "episode reward: 0.0 , episode terminated after 891 env steps\n",
      "avg_reward after 98 (out of 100) episodes: 0.35714285714285715\n",
      "episode 98\n",
      "episode reward: 0.0 , episode terminated after 3910 env steps\n",
      "avg_reward after 99 (out of 100) episodes: 0.35353535353535354\n",
      "episode 99\n",
      "episode reward: 0.0 , episode terminated after 809 env steps\n",
      "avg_reward after 100 (out of 100) episodes: 0.35\n",
      "Total avg_reward: 0.35\n"
     ]
    }
   ],
   "source": [
    "assert eval_policy_path is not None\n",
    "\n",
    "agent.load(eval_policy_path, eval_policy_model_id)\n",
    "\n",
    "print(f\"loaded network {eval_policy_path} {eval_policy_model_id}\")\n",
    "\n",
    "policy = agent.act\n",
    "\n",
    "with open(p_join(OUTPUT_DIR, \"status.txt\"), 'w') as status_file:\n",
    "    status_file.write('running test_policy')\n",
    "\n",
    "if test:\n",
    "    args.eval_policy_episodes = 2\n",
    "\n",
    "test_policy(writer, env, policy, img, vec, eval_policy_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "208a2efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046d5ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:venv] *",
   "language": "python",
   "name": "conda-env-venv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
