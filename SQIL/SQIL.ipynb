{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02ea9230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(400, 300))\n",
    "display.start();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c0facc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankitagarg/.local/lib/python3.6/site-packages/gym/logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "import minerl\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import random\n",
    "\n",
    "from torch import nn\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "from minerl.data import BufferedBatchIter\n",
    "from collections import deque\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32ebc6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import replay_memory\n",
    "from src import dueling_network as network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad6518a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_buffer(online_memory_replay, expert_memory_replay, online_memory_batch_size, expert_memory_batch_size):\n",
    "  \n",
    "    expert_batch = expert_memory_replay.sample(expert_memory_batch_size)\n",
    "    expert_batch_state = []\n",
    "    expert_batch_action = [] \n",
    "    expert_batch_next_state = [] \n",
    "    expert_batch_reward = [] \n",
    "    expert_batch_done = []\n",
    "\n",
    "    for x in expert_batch:\n",
    "#         for x in t:\n",
    "        expert_batch_state.append(x[0]) \n",
    "        expert_batch_action.append(x[1]) \n",
    "        expert_batch_next_state.append(x[3]) \n",
    "        expert_batch_reward.append(x[2]) \n",
    "        expert_batch_done.append(x[4])\n",
    "            \n",
    "    expert_batch_state = np.array([expert_batch_state[i][\"pov\"] for i in range(len(expert_batch_state))])\n",
    "    expert_batch_state = torch.from_numpy(expert_batch_state.transpose(0, 3, 1, 2).astype(np.float32) / 255)\n",
    "\n",
    "    expert_batch_next_state = np.array([expert_batch_next_state[i][\"pov\"] for i in range(len(expert_batch_next_state))])\n",
    "    expert_batch_next_state = torch.from_numpy(expert_batch_next_state.transpose(0, 3, 1, 2).astype(np.float32) / 255)\n",
    "\n",
    "    expert_batch_action = torch.Tensor([expert_batch_action[i] for i in range(len(expert_batch_action))])\n",
    "    expert_batch_reward = torch.Tensor([expert_batch_reward[i] for i in range(len(expert_batch_reward))]).unsqueeze(1)\n",
    "    expert_batch_done = torch.Tensor([expert_batch_done[i] for i in range(len(expert_batch_done))]).unsqueeze(1)\n",
    "\n",
    "    if online_memory_replay.size() == 0:\n",
    "        return expert_batch_state, expert_batch_action, expert_batch_reward, expert_batch_next_state, expert_batch_done\n",
    "\n",
    "    \n",
    "    online_batch = online_memory_replay.sample(online_memory_batch_size)\n",
    "    online_batch_state = []\n",
    "    online_batch_action = [] \n",
    "    online_batch_next_state = [] \n",
    "    online_batch_reward = [] \n",
    "    online_batch_done = []\n",
    "    for x in online_batch:\n",
    "#         for x in t:\n",
    "        online_batch_state.append(x[0]) \n",
    "        online_batch_action.append(x[1]) \n",
    "        online_batch_next_state.append(x[3]) \n",
    "        online_batch_reward.append(x[2]) \n",
    "        online_batch_done.append(x[4])\n",
    "\n",
    "    online_batch_state = np.array([online_batch_state[i][\"pov\"] for i in range(len(online_batch_state))])\n",
    "    online_batch_state = torch.from_numpy(online_batch_state.transpose(0, 3, 1, 2).astype(np.float32) / 255)\n",
    "\n",
    "    online_batch_next_state = np.array([online_batch_next_state[i][\"pov\"] for i in range(len(online_batch_next_state))])\n",
    "    online_batch_next_state = torch.from_numpy(online_batch_next_state.transpose(0, 3, 1, 2).astype(np.float32) / 255)\n",
    "\n",
    "    online_batch_action = torch.Tensor([online_batch_action[i] for i in range(len(online_batch_action))]).unsqueeze(1)\n",
    "    online_batch_reward = torch.Tensor([online_batch_reward[i] for i in range(len(online_batch_reward))]).unsqueeze(1)\n",
    "    online_batch_done = torch.Tensor([online_batch_done[i] for i in range(len(online_batch_done))]).unsqueeze(1)\n",
    "\n",
    "    batch_state = torch.cat([online_batch_state, expert_batch_state], dim=0)\n",
    "    batch_next_state = torch.cat([online_batch_next_state, expert_batch_next_state], dim=0)\n",
    "    batch_action = torch.cat([online_batch_action, expert_batch_action], dim=0)\n",
    "    batch_reward = torch.cat([online_batch_reward, expert_batch_reward], dim=0)\n",
    "    batch_done = torch.cat([online_batch_done, expert_batch_done], dim=0)\n",
    "\n",
    "    return batch_state, batch_action, batch_reward, batch_next_state, batch_done\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86e3b588",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_centroids = np.load('./action_centroids.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0335726",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/\" #directory containing mineral human demonstration data \n",
    "ENVIRONMENT = 'MineRLTreechopVectorObf-v0'\n",
    "NUM_ACTION_CENTROIDS = 64\n",
    "ONLINE_REPLAY_MEMORY = 20000\n",
    "EXPERT_REPLAY_MEMORY = 40000\n",
    "\n",
    "writer = SummaryWriter('logs/sqil')\n",
    "data = minerl.data.make(ENVIRONMENT, data_dir=DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a83fdd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, input_shape, output_dim, alpha):\n",
    "        super(Policy, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        n_input_channels = input_shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with torch.no_grad():\n",
    "            n_flatten = self.cnn(torch.zeros(1, *input_shape)).shape[1]\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(n_flatten, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(self.cnn(x))\n",
    "\n",
    "    def getV(self, q_value):\n",
    "        v = self.alpha * torch.log(torch.sum(torch.exp(q_value/self.alpha), dim=1, keepdim=True))\n",
    "        return v\n",
    "        \n",
    "    def choose_action(self, state, epsilon):\n",
    "        state = torch.FloatTensor(state)\n",
    "        # print('state : ', state)\n",
    "        with torch.no_grad():\n",
    "            q = self.forward(state)\n",
    "            v = self.getV(q).squeeze()\n",
    "            dist = torch.exp((q-v)/self.alpha)\n",
    "            dist = dist / torch.sum(dist)\n",
    "            if epsilon < random.uniform(0, 1):\n",
    "                a = torch.argmax(dist)\n",
    "            else:\n",
    "                c = Categorical(dist)\n",
    "                a = c.sample()\n",
    "        return a.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dae9ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(network, action_centroids, num_episodes):\n",
    "    \n",
    "    num_actions = action_centroids.shape[0]\n",
    "    action_list = np.arange(num_actions)\n",
    "\n",
    "    episode_rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        while not done:\n",
    "            network_state = torch.from_numpy(obs['pov'].transpose(2, 0, 1)[None].astype(np.float32) / 255)\n",
    "            selected_action = network.choose_action(network_state, 0)\n",
    "            action = action_centroids[selected_action]\n",
    "            minerl_action = {\"vector\": action}\n",
    "\n",
    "            obs, reward, done, info = env.step(minerl_action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            if steps > 18000: \n",
    "                break\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "    \n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9cd6a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "onlineQNetwork = network.SoftQNetwork(NUM_ACTION_CENTROIDS, 3)\n",
    "targetQNetwork = network.SoftQNetwork(NUM_ACTION_CENTROIDS, 3)\n",
    "targetQNetwork.load_state_dict(onlineQNetwork.state_dict())\n",
    "env = gym.make(ENVIRONMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5346cfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_memory_replay = replay_memory.Memory(EXPERT_REPLAY_MEMORY)\n",
    "expert_memory_replay.load('expert_memory_replay')\n",
    "online_memory_replay = replay_memory.Memory(ONLINE_REPLAY_MEMORY)\n",
    "online_memory_replay.load('online_memory_replay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fa47fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_EPOCHS = 100\n",
    "NUM_STEPS = 200 \n",
    "GAMMA = 0.99\n",
    "REPLAY_START_SIZE = 5000 \n",
    "\n",
    "epsilon = 1 #0.999\n",
    "decay = 0.9\n",
    "update_steps = 3000\n",
    "batch_size = 32\n",
    "min_epsilon = 0.1\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f830d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_steps = 0\n",
    "begin_learn = False\n",
    "training_loss = []\n",
    "training_qvalue = []\n",
    "training_return = []\n",
    "    \n",
    "for epoch in range(NUM_OF_EPOCHS):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    loss_values = []\n",
    "    q_values = []\n",
    "    for time_steps in range(NUM_STEPS):\n",
    "        network_state = torch.from_numpy(state['pov'].transpose(2, 0, 1)[None].astype(np.float32) / 255)\n",
    "        selected_action = onlineQNetwork.choose_action(network_state, epsilon)\n",
    "        action = action_centroids[selected_action]\n",
    "        minerl_action = {\"vector\": action}\n",
    "\n",
    "        next_state, reward, done, _ = env.step(minerl_action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        online_memory_replay.add((state, selected_action, 0, next_state, done))\n",
    "\n",
    "        if online_memory_replay.size() > REPLAY_START_SIZE:\n",
    "            if begin_learn is False:\n",
    "                print('learn begin!')\n",
    "                begin_learn = True\n",
    "            learn_steps += 1\n",
    "            if learn_steps % update_steps == 0:\n",
    "                print('updating target network')\n",
    "                targetQNetwork.load_state_dict(onlineQNetwork.state_dict())\n",
    "\n",
    "            batch_state, batch_action, batch_reward, batch_next_state, batch_done = sample_from_buffer(online_memory_replay, expert_memory_replay, batch_size, batch_size)\n",
    "\n",
    "\n",
    "            with torch.no_grad():\n",
    "                next_q = targetQNetwork(batch_next_state)\n",
    "                next_v = targetQNetwork.getV(next_q)\n",
    "                y = batch_reward + (1 - batch_done) * GAMMA * next_v\n",
    "\n",
    "            loss = F.huber_loss(onlineQNetwork(batch_state).gather(1, batch_action.long()), y)\n",
    "            optimizer = torch.optim.Adam(onlineQNetwork.parameters(), lr=learning_rate)\n",
    "            \n",
    "            loss_values.append(loss.item())\n",
    "            q_values.append(torch.mean(y).item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            for param in onlineQNetwork.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (learn_steps % 100) == 0:\n",
    "                print('learn_step: ', learn_steps,' loss: ', np.mean(np.array(loss_values)), ' q_value: ', np.mean(np.array(q_values)))\n",
    "                training_loss.append(np.mean(np.array(loss_values)))\n",
    "                training_qvalue.append(np.mean(np.array(q_values)))\n",
    "            \n",
    "            if (learn_steps % 10000) == 0:\n",
    "                return_value = test(targetQNetwork, action_centroids, 5)\n",
    "                training_return.append(np.mean(np.array(return_value)))\n",
    "                print('training return', np.mean(np.array(return_value)))\n",
    "                writer.add_scalar('training return', np.mean(np.array(return_value)), global_step=learn_steps)\n",
    "            \n",
    "            writer.add_scalar('training loss', loss.item(), global_step=learn_steps)\n",
    "            writer.add_scalar('training q-value', torch.mean(y), global_step=learn_steps)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        state = next_state\n",
    "    \n",
    "#     epsilon = max(min_epsilon, epsilon*decay)\n",
    "    torch.save(onlineQNetwork, 'sqil-policy-2.pth')\n",
    "    online_memory_replay.save('online_memory_replay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d870fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(targetQNetwork, 'target_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c198ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:venv] *",
   "language": "python",
   "name": "conda-env-venv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
